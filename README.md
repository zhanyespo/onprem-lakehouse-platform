# ğŸ§± Local Lakehouse Platform (Streaming Â· Analytics Â· ML)

A **production-style, open-source data platform** running fully **locally** using  
**WSL2 + Docker + VS Code**, designed to mirror modern cloud architectures while remaining
portable, reproducible, and vendor-neutral.

This repository implements an **end-to-end lakehouse** with streaming ingestion, object storage,
distributed compute, analytics warehouse, orchestration, ML lifecycle, and BI â€” all running on a single machine.

---

## ğŸ§  Architecture Overview

```

Sources
â†“
[ Ingestion ]
MQTT / Kafka / NiFi
â†“
[ Storage ]
MinIO (Parquet / Delta / Iceberg)
â†“
[ Compute ]
Spark / Flink / dbt
â†“
[ Warehouse ]
ClickHouse + Trino
â†“
[ BI / ML ]
Superset / Metabase / MLflow

````

Orchestration: **Airflow**  
Infrastructure: **Docker â†’ Kubernetes (future)**

---

## ğŸ¯ Goals

- Reproduce **cloud-native data architecture locally**
- Support **batch + streaming workloads**
- Separate **infrastructure, compute, and data logic**
- Enable **analytics, ML, and BI** from the same data foundation
- Remain **vendor-agnostic** and fully open-source

---

## ğŸ“ Repository Structure

```text
lakehouse-platform/
â”œâ”€â”€ infra/                 # Infrastructure definitions (Docker / K8s)
â”œâ”€â”€ ingestion/             # Kafka, MQTT, NiFi pipelines
â”œâ”€â”€ storage/               # Bronze / Silver / Gold lake layers
â”œâ”€â”€ compute/               # Spark, Flink, dbt jobs
â”œâ”€â”€ warehouse/             # ClickHouse & Trino SQL
â”œâ”€â”€ orchestration/         # Airflow DAGs
â”œâ”€â”€ ml/                    # ML training, experiments, inference
â”œâ”€â”€ bi/                    # Superset & Metabase configs
â”œâ”€â”€ data/                  # Sample / test datasets
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
````

---

## ğŸ§± Stack Components

### Ingestion

* **Kafka** â€” streaming event ingestion
* **MQTT** â€” lightweight IoT / edge events
* **NiFi** â€” optional visual flow orchestration

### Storage

* **MinIO** â€” S3-compatible object storage
* **Parquet** â€” columnar storage
* **Delta / Iceberg** â€” table formats (Silver / Gold)

### Compute

* **Spark** â€” batch ETL & transformations
* **Flink** â€” real-time stream processing
* **dbt** â€” analytics transformations

### Warehouse

* **ClickHouse** â€” high-performance OLAP
* **Trino** â€” federated SQL across sources

### Orchestration

* **Airflow** â€” scheduling, dependency management

### Machine Learning

* **MLflow** â€” experiment tracking & model registry
* **Python** â€” training & inference pipelines

### BI

* **Apache Superset**
* **Metabase**

### Infrastructure

* **Docker Compose** (local)
* **Kubernetes** (planned)

---

## ğŸ—‚ï¸ Data Lake Layout

```text
s3://lake/
â”œâ”€â”€ bronze/      # raw ingested data
â”œâ”€â”€ silver/      # cleaned / normalized
â””â”€â”€ gold/        # analytics-ready datasets
```

---

## ğŸš€ Getting Started

### Prerequisites

* Windows 10/11
* WSL2 (Ubuntu recommended)
* Docker Desktop
* VS Code + Remote WSL extension

---

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/your-org/lakehouse-platform.git
cd lakehouse-platform
```

---

### 2ï¸âƒ£ Configure Environment

```bash
cp .env.example .env
```

Edit `.env` as needed (ports, credentials, resource limits).

---

### 3ï¸âƒ£ Start Platform

```bash
docker compose up -d
```

Verify services:

| Service    | URL                                            |
| ---------- | ---------------------------------------------- |
| MinIO      | [http://localhost:9001](http://localhost:9001) |
| Airflow    | [http://localhost:8080](http://localhost:8080) |
| Superset   | [http://localhost:8088](http://localhost:8088) |
| Metabase   | [http://localhost:3000](http://localhost:3000) |
| MLflow     | [http://localhost:5000](http://localhost:5000) |
| ClickHouse | [http://localhost:8123](http://localhost:8123) |

---

## ğŸ› ï¸ Development Workflow

* **All code mounted as volumes** â€” no image rebuilds during development
* **SQL lives with the warehouse**
* **Business logic lives in compute**
* **Airflow DAGs only orchestrate**

Recommended VS Code extensions:

* Docker
* Python
* SQLTools
* dbt Power User

---

## ğŸ§ª Example Pipelines

* Kafka â†’ MinIO (Bronze)
* Spark â†’ Silver transforms
* ClickHouse materialized views
* dbt â†’ Gold semantic layer
* Airflow DAG orchestration
* MLflow experiment tracking
* Superset dashboards

---

## ğŸ“ˆ Roadmap

* [ ] Iceberg tables with Trino
* [ ] Flink real-time metrics
* [ ] Kubernetes deployment
* [ ] CI/CD for data pipelines
* [ ] Data quality checks
* [ ] Feature store patterns

---

## ğŸ§© Design Principles

* Infrastructure as code
* Idempotent pipelines
* Clear layer separation
* Cloud-portable design
* Analytics & ML from same source of truth

---

## ğŸ“œ License

MIT License â€” free to use, modify, and extend.

---

## ğŸ™Œ Why This Project Exists

This platform is built to **learn, test, and demonstrate modern data engineering architecture**
without relying on managed cloud services â€” while remaining production-realistic.

It is suitable for:

* learning
* experimentation
* portfolio projects
* architectural prototyping